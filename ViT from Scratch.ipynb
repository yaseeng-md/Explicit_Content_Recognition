{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb6f0e8d-643f-4ed6-bcdf-939c6d416e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from einops import repeat\n",
    "import torch \n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "from random import random\n",
    "from torchvision.transforms import Resize, ToTensor\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "to_tensor = [Resize((244,244)),ToTensor()]\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f628a788-5c1a-44d6-99cb-198307869d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops.layers.torch import Rearrange\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8888bef0-1f51-41f0-a479-bf86c0edf253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class patch_Embedding(nn.Module):\n",
    "    def __init__(self,in_channels,emb_size = 128,patch_size = 8):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.emb_size = emb_size\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)',p1 = patch_size, p2 = patch_size),\n",
    "            nn.Linear(in_features = (patch_size * patch_size * in_channels), out_features = self.emb_size)\n",
    "        )\n",
    "    def forward(self,x : Tensor):\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2375d1df-0bc6-4031-9082-7624fd55ce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attension(nn.Module):\n",
    "    def __init__(self,dim,n_heads,dropout):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.attension = nn.MultiheadAttention(embed_dim=dim,num_heads=self.n_heads,dropout=dropout)\n",
    "        self.q = nn.Linear(dim,dim)\n",
    "        self.k = nn.Linear(dim,dim)\n",
    "        self.v = nn.Linear(dim,dim)\n",
    "    def forward(self,x):\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        attention_output, attention_weight = self.attension(q,k,v)\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7de2aec-9f3c-4e3b-9127-8586689741c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self,dim,fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.dim = dim\n",
    "        self.norm = nn.LayerNorm(self.dim)\n",
    "    def forward(self,x,**kwargs):\n",
    "        return self.fn(self.norm(x),**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba5662e0-0646-4804-9cf5-55fe238e462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00301be4-0efa-4a7a-ad44-9a15d5a9e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self,fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    def forward(self,x,**kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x,**kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b31c6394-8708-4151-b3c2-418bd93595f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self,ch=3,img_size = 224, patch_size = 16, emb_size = 32, n_layers = 7, output_dim = 1, dropout = 0.1, heads = 2):\n",
    "        super(ViT,self).__init__()\n",
    "\n",
    "        # Attributes\n",
    "        self.channels = ch\n",
    "        self.img_size = img_size\n",
    "        self.width = img_size\n",
    "        self.width = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.emb_size = emb_size\n",
    "        self.n_layers = n_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.n_heads = heads\n",
    "\n",
    "        # Patching\n",
    "        self.patch_embedding = patch_Embedding(in_channels=self.channels, emb_size = self.emb_size, patch_size = self.patch_size)\n",
    "\n",
    "        # Patching + Positional Embedding + CLS Token\n",
    "        num_patches = (self.img_size // self.patch_size) ** 2\n",
    "            \n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, self.emb_size))\n",
    "        self.cls_token = nn.Parameter(torch.rand(1,1,self.emb_size))\n",
    "\n",
    "        # Transformer Encoders\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range (self.n_layers):\n",
    "            transformer_block = nn.Sequential(\n",
    "                ResidualAdd(PreNorm(self.emb_size, Attension(emb_size, n_heads=self.n_heads, dropout = self.dropout))),\n",
    "                ResidualAdd(PreNorm(self.emb_size, FeedForward(self.emb_size,self.emb_size, dropout = self.dropout)))\n",
    "            )\n",
    "            self.layers.append(transformer_block)\n",
    "\n",
    "        # Classification Head\n",
    "        self.head = nn.Sequential(nn.LayerNorm(self.emb_size), nn.Linear(emb_size, output_dim))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self,img):\n",
    "        x = self.patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "        cls_token = repeat(self.cls_token, \"1 1 d -> b 1 d\", b = b)\n",
    "        x = torch.cat((cls_token,x),dim = 1)\n",
    "        x += self.pos_embedding[:,:(n+1)]\n",
    "\n",
    "        # Transformer Layers\n",
    "        for i in range (self.n_layers):\n",
    "            x = self.layers[i](x)\n",
    "        return self.head(x[:,0,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ffe50dba-861c-49a7-9208-287729d74fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9167]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViT()\n",
    "model(torch.ones((1,3,224,224)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
